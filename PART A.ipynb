{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81dc2d4-d434-41f9-a46a-c36d879826fe",
   "metadata": {},
   "source": [
    "## Assignment: Associate Data Scientist  \n",
    "**Candidate:** Karishma Rawal  \n",
    "**Position Applied:** Associate Data Scientist üìä  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898036a5-8966-4ec0-8441-406a4a44f051",
   "metadata": {},
   "source": [
    "### PROBLEM STATEMENT\n",
    "- Dataset (attached with the task): The data contains a pair of paragraphs. These text paragraphs are\n",
    "randomly sampled from a raw dataset. Each pair of sentences may or may not be semantically similar.\n",
    "The candidate is to predict a value between 0-1 indicating the similarity between the pair of text paras.\n",
    "A sample of a similar dataset will be used as test data, therefore it‚Äôs crucial to the model solution using\n",
    "provided dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58138f92-efa3-47da-a8f1-4c2719c80780",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "- Build an algorithm/model that can quantify the degree of similarity between the two text-based on Semantic similarity. Semantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other.\n",
    "- 1 means highly similar\n",
    "- 0 means highly dissimilar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9e985-21fd-4067-989d-c04379b6f498",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb2d865-6920-4d02-a459-4c9180f20c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb4e30-308e-4be8-850a-80c231899508",
   "metadata": {},
   "source": [
    "#### 2. Reading the CSV file into a Pandas DataFrame üìÇ‚û°Ô∏èüìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba63be4e-a9a7-49c7-8c7f-65fae142ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('DataNeuron_Text_Similarity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad018a-08bf-4188-b457-9c0c1bdfc6f8",
   "metadata": {},
   "source": [
    "#### 3. Checking the first few rows to understand the structure of the data üßêüîç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d034d8fc-a2ad-44ec-8d5f-bd88842a68c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenges tv viewing the number of ...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap boss arrested over drug find rap mogul mar...</td>\n",
       "      <td>amnesty chief laments war failure the lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burn-out worries robinson england coach...</td>\n",
       "      <td>hanks greeted at wintry premiere hollywood sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hearts of oak 3-2 cotonsport hearts of oak set...</td>\n",
       "      <td>redford s vision of sundance despite sporting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir paul rocks super bowl crowds sir paul mcca...</td>\n",
       "      <td>mauresmo opens with victory in la amelie maure...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  broadband challenges tv viewing the number of ...   \n",
       "1  rap boss arrested over drug find rap mogul mar...   \n",
       "2  player burn-out worries robinson england coach...   \n",
       "3  hearts of oak 3-2 cotonsport hearts of oak set...   \n",
       "4  sir paul rocks super bowl crowds sir paul mcca...   \n",
       "\n",
       "                                               text2  \n",
       "0  gardener wins double in glasgow britain s jaso...  \n",
       "1  amnesty chief laments war failure the lack of ...  \n",
       "2  hanks greeted at wintry premiere hollywood sta...  \n",
       "3  redford s vision of sundance despite sporting ...  \n",
       "4  mauresmo opens with victory in la amelie maure...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02016cf6-9f04-43d8-9cd7-e56e386bb632",
   "metadata": {},
   "source": [
    "####  4. Getting an Overview of the Dataset üìäüîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc631c4-0696-434a-81c7-1d9776377f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text1   3000 non-null   object\n",
      " 1   text2   3000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf07456-9c55-428c-81d7-2b1353719656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text1    object\n",
       "text2    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeddd2f-89d9-486d-8162-e7708dd4e561",
   "metadata": {},
   "source": [
    "#### 5. Checking the shape of the dataset üìèüîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "466e5208-8861-4c86-88f0-8720769726f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:3000 \n",
      " Columns:2\n"
     ]
    }
   ],
   "source": [
    "Rows,Columns=data.shape\n",
    "print(f'Rows:{Rows} \\n Columns:{Columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9088a30-28d0-4e08-945e-b4bba2c9e458",
   "metadata": {},
   "source": [
    "#### 6. Checking Missing Values üîç‚ùì‚û°Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ead2a9-bb89-416a-ad19-b4d11fd7a5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text1    0\n",
       "text2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e553a-5d4d-436f-99a2-8d7a22f6cb13",
   "metadata": {},
   "source": [
    "#### 7. üîç‚ú® Checking for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a40c2e0-6a4d-4b5a-b9ba-5bebadbacb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5acc5c84-a4e4-4e37-a890-564583a03e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef4a1e-efa1-4edf-89cc-0637927b0376",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf272b8-7d72-4bec-b7f8-6ce1e9935ac4",
   "metadata": {},
   "source": [
    "#### 8.üîΩ‚ú® Downloading NLTK Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8f18f0-b90f-4f37-9c30-d2ad6420a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KARISHMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KARISHMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KARISHMA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fcb54-26f4-4ff1-b1d3-0ba2317be0c3",
   "metadata": {},
   "source": [
    "#### 9.üöÄ‚ú® Initializing Lemmatizer and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5417dfa-d365-4db9-b39e-cb17a4f1ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5cbea-69df-47dc-b15a-a56e136a36e5",
   "metadata": {},
   "source": [
    "#### 10.üìù‚ú® Text Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0fbc2ca-320a-43bb-b172-1d97f56e3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)   \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fd2e4-71fe-40ff-b4d2-b4a1c404d75c",
   "metadata": {},
   "source": [
    "#### 11.üîÑ‚ú® Applying Preprocessing to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab7e78c-eaaf-41a4-89c4-7e6030b47b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text1'] = data['text1'].apply(preprocess)\n",
    "data['text2'] = data['text2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c34a87-245c-4039-af89-12e2103e824f",
   "metadata": {},
   "source": [
    "#### 12.üî¢‚ú® Converting Text to TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa2b96f-fb2d-4b09-af2b-4ef896de0c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(data['text1'].tolist() + data['text2'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15f0c4-f5c3-4b2b-a0fa-6a76c43a5a9e",
   "metadata": {},
   "source": [
    "#### 13.üî¢‚ú® Splitting TF-IDF Matrix into Text1 and Text2 Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31a8680-9e4a-4d2f-8a19-47e75e63a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text1 = tfidf_matrix[:len(data)]\n",
    "tfidf_text2 = tfidf_matrix[len(data):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5930c33-3b59-4fdd-ada7-0a2a6ce812c3",
   "metadata": {},
   "source": [
    "#### 14.üî¢‚ú® Computing Cosine Similarity Between Text1 and Text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcd0b88e-f82f-406d-aeda-dc4b3253ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text1  \\\n",
      "0  broadband challenge tv viewing number european...   \n",
      "1  rap bos arrested drug find rap mogul marion su...   \n",
      "2  player burnout worry robinson england coach an...   \n",
      "3  heart oak cotonsport heart oak set ghanaian co...   \n",
      "4  sir paul rock super bowl crowd sir paul mccart...   \n",
      "\n",
      "                                               text2  similarity_score  \n",
      "0  gardener win double glasgow britain jason gard...          0.066590  \n",
      "1  amnesty chief lament war failure lack public o...          0.004893  \n",
      "2  hank greeted wintry premiere hollywood star to...          0.010037  \n",
      "3  redford vision sundance despite sporting cordu...          0.013939  \n",
      "4  mauresmo open victory la amelie mauresmo maria...          0.023245  \n"
     ]
    }
   ],
   "source": [
    "data['similarity_score'] = [cosine_similarity(tfidf_text1[i], tfidf_text2[i])[0][0] for i in range(len(data))]\n",
    "\n",
    "print(data[['text1', 'text2', 'similarity_score']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad120fe3-d7f3-4325-bbe2-889f40e459af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the model for deployment\n",
    "with open(\"similarity_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Saving the predictions\n",
    "data.to_csv(\"predicted_similarity_scores.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
